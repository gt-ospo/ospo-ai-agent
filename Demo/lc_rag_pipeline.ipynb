{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os, torch, json, re, numpy as np\n",
    "\n",
    "# local model instantiation\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# agent\n",
    "from langchain import hub\n",
    "from langchain.agents import create_structured_chat_agent, AgentExecutor\n",
    "from langchain_community.llms.utils import enforce_stop_tokens\n",
    "from langchain_core.runnables import Runnable\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qwen3 setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What create_react_agent expects \n",
    "\n",
    "uses LangChain's ReAct output parser \"ReActSingleInputOutputParser.\" This parser is very strict. Every LLM turn must be structured as one of the following:\n",
    "\n",
    "Thought: ...\n",
    "Action: <tool_name>\n",
    "Action Input: ...\n",
    "or\n",
    "\n",
    "Thought: ...\n",
    "Final Answer: ...\n",
    "not both. Both Qwen3-1.7B and Qwen3-8B sometimes produce both an Action and Final Answer in one message, causing the parser to throw an OutputParserException. This looks like:\n",
    "\n",
    "```\n",
    "OutputParserException: Parsing LLM output produced both a final answer and a parse-able action.\n",
    "```\n",
    "\n",
    "### How regex pattern matching work's in our pipeline\n",
    "\n",
    "1. re.escape: Scans a string and prefixes every character that has a special meaning in regular expression syntax with a backslash (`. ^ $ * + ? { } [ ] \\). This is done so that these metacharacters aren't compiled into regex pattern matching before the text is searched.\n",
    "\n",
    "*WITHOUT* using re.escape(\"1+\"):\n",
    "\n",
    "```python\n",
    ">>> import re\n",
    ">>> text = \"Here’s a math fact: 1+11=2? Cool, right?\"\n",
    ">>> re.split(\"1+\", text) # match one or more \"1\"s\n",
    "[\"Here’s a math fact: \", \"+\", \"=2? Cool, right?\"]\n",
    "```\n",
    "\n",
    "*USING* re.escape(\"1+\"):\n",
    "\n",
    "```python\n",
    ">>> import re\n",
    ">>> text = \"Here’s a math fact: 1+11=2? Cool, right?\"\n",
    ">>> re.split(re.escape(\"1+\"), text) # now we're searching for the literal \"1\\\\+\"\n",
    "[\"Here’s a math fact: \", \"11=2? Cool, right?\"]\n",
    "```\n",
    "\n",
    "2. ```map(re.escape, self.stop)```: applies re.escape to each stop string\n",
    "3. ```\"|\".join(map(re.escape, self.stop))```: joins the escaped stop tokens with |, producing a regex “or” expression. If self.stop = [\"Obs:\", \"Observation:\"], you’d get r\"Obs:|Observation:\"\n",
    "4. ```re.split(pattern, text)```: actually splits the text at each pattern\n",
    "\n",
    "Example:\n",
    "\n",
    "```text = (\n",
    "    \"The chemical reaction proceeded without incident. \"\n",
    "    \"Observation: the solution turned bright blue. \"\n",
    "    \"We collected three samples for further analysis.\"\n",
    ")\n",
    "```\n",
    "produces\n",
    "\n",
    "```[\n",
    "  \"The chemical reaction proceeded without incident. \",\n",
    "  \" the solution turned bright blue. We collected three samples for further analysis.\"\n",
    "]\n",
    "```\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "The below method using the shadow class for HuggingFacePipeline follows the streaming approach:\n",
    "\n",
    "Converts each stop string to its token-ID, wraps it in a custom StoppingCriteria, and passes that into model.generate before generation starts. The moment the last token of any stop string appears, generation terminates — no extra tokens are produced, saving compute/time.\n",
    "\n",
    "<u>Section refs</u>:\n",
    "\n",
    "https://python.langchain.com/api_reference/huggingface/llms/langchain_huggingface.llms.huggingface_pipeline.HuggingFacePipeline.html\n",
    "\n",
    "source code for huggingFace pipeline:\n",
    "\n",
    "https://python.langchain.com/api_reference/_modules/langchain_huggingface/llms/huggingface_pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"/storage/ice1/6/7/dharden7/rag_models/Qwen3-8B\"\n",
    "\n",
    "# trust_remote_code flag allows from_pretrained func to execute custom class definitions\n",
    "# bcs qwen is not native to AutoModelForCausalLM compiler\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "max_tokens = 512\n",
    "temp = 0.1\n",
    "top_p = 0.9\n",
    "\n",
    "gen_cfg = dict(max_new_tokens=max_tokens, temperature=temp, top_p=top_p)\n",
    "\n",
    "# create an hf text gen pipeline \n",
    "hf_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, **gen_cfg)\n",
    "\n",
    "# We want the LLM pipeline to stop generating tokens after \"Observation:\", so that \n",
    "# the tool call reaches the LLM and is not hallucinated.\n",
    "STOP_STRINGS = [\"\\nObservation:\", \"\\n[ANSWER END]\"]\n",
    "\n",
    "# shadow HuggingFacePipeline to fix _trim class - see stack overflow \n",
    "class hf_stop(HuggingFacePipeline):\n",
    "\n",
    "    def __init__(self, pipeline, stop=None, **kwargs):\n",
    "        super().__init__(pipeline=pipeline, **kwargs)\n",
    "\n",
    "        # Normally, Pydantic complains if we define attributes after super().__init__\n",
    "        # bypass pydantic and assign stop to underlying dict\n",
    "        object.__setattr__(self, \"stop\", stop)\n",
    "\n",
    "    def _trim(self, text: str) -> str:\n",
    "        pattern = \"|\".join(map(re.escape, self.stop))\n",
    "        return re.split(pattern, text)[0]\n",
    "\n",
    "    def invoke(self, input, config=None, **kwargs):\n",
    "        text = super().invoke(input, **kwargs)\n",
    "        return self._trim(text)\n",
    "\n",
    "    '''\n",
    "    Iterate over each token-chunk produced by the wrapped LLMs own stream() method.\n",
    "    A chunk here is a string such as \"The\", \"dog\", \"barked\", depending on tokenization.\n",
    "    Whatever is yeilded here is sent to the AgentExecutor.\n",
    "    '''\n",
    "    def stream(self, input, config=None, **kwargs):\n",
    "        acc = \"\"\n",
    "        for chunk in super().stream(input, config=config, **kwargs):\n",
    "            acc += chunk\n",
    "            trimmed = self._trim(acc)\n",
    "            if trimmed != acc:\n",
    "                # len(acc) - len(chunk) is the buffer length before adding this chunk\n",
    "                # hence we return everything accumulated up to the latest chunk\n",
    "                yield trimmed[len(acc) - len(chunk):]\n",
    "                break\n",
    "            yield chunk\n",
    "\n",
    "llm = hf_stop(pipeline=hf_pipe, stop=STOP_STRINGS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# quick sanity test\n",
    "if DEBUG:\n",
    "    print(llm.invoke(\"Thought: X\\nAction: Y\\nAction Input: Z\\nObservation: SHOULD NOT APPEAR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative but slower approach to stop_tokens\n",
    "\n",
    "When you call llm.bind(stop=[...]).invoke(...) (the non-streaming pathway) the model still generates the whole completion; LangChain just slices the returned string afterward. Only the streaming methods (stream()/astream()) actually halt token generation mid-flight.\n",
    "\n",
    "#### Why this is less efficient\n",
    "\n",
    "With **llm.bind(...).invoke() you’re paying for the full 512-token generation—even though you only keep the prefix—whereas with **stream() you’d stop the GPU as soon as \"Observation:\" is emitted.\n",
    "\n",
    "<u>Relevant documentation</u>:\n",
    "\n",
    "1. https://python.langchain.com/docs/how_to/streaming/\n",
    "2. https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html\n",
    "3. https://python.langchain.com/docs/how_to/binding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    stop = [\"\\nObservation\"] \n",
    "\n",
    "    llm_to_use = llm.bind(\n",
    "        stop=stop,                  \n",
    "        max_new_tokens=max_tokens,  \n",
    "        temperature=temp,           \n",
    "        top_p=top_p,                \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# quick sanity test\n",
    "if DEBUG:\n",
    "    print(llm_to_use.invoke(\"Thought: X\\nAction: Y\\nAction Input: Z\\nObservation: SHOULD NOT APPEAR\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma on disk persistent vector DB\n",
    "\n",
    "<u>Relevant documentation</u>:\n",
    "\n",
    "1. https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface.HuggingFaceEmbeddings.html\n",
    "2. https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# general - DEBUG\n",
    "\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"FALSE\"\n",
    "os.environ[\"LANGCHAIN_TELEMETRY\"] = \"false\"\n",
    "\n",
    "# vector store\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import logging, warnings\n",
    "from chromadb.config import Settings\n",
    "import chromadb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Telemetry?\n",
    "\n",
    "In this context, telemetry refers to Chroma’s built-in instrumentation that automatically collects and (normally) reports anonymous usage data back to its maintainers. Typical telemetry events include things like:\n",
    "\n",
    "* ClientStartEvent – “My library has just been initialized.”\n",
    "\n",
    "* ClientCreateCollectionEvent – “A new collection (i.e. a new vector index) was created.”\n",
    "\n",
    "* CollectionQueryEvent, CollectionGetEvent, etc. – “The client ran a query” or “fetched some documents.”\n",
    "\n",
    "The idea is to help the Chroma developers understand how the library is being used in the wild—e.g. what features are popular, how often people hit the disk, what query volumes look like, and so on.\n",
    "\n",
    "### How is it used in Chromadb?\n",
    "\n",
    "Chromadb ships with a “no-op” telemetry function\n",
    "In order to avoid pulling in a full telemetry dependency, the Python side of Chromadb provides its own capture function that simply does nothing. But it was implemented to accept one argument (say, an event object).\n",
    "\n",
    "The Rust backend calls capture() with three arguments\n",
    "When you initialize the client or create a collection, the Rust code does something like:\n",
    "\n",
    "```python\n",
    "// pseudocode\n",
    "Python.capture(\"ClientStartEvent\", payload_dict, metadata_dict)\n",
    "```\n",
    "\n",
    "That ends up invoking the Python stub as capture(arg1, arg2, arg3), but since that stub only takes one parameter, Python immediately raises:\n",
    "\n",
    "```python\n",
    "capture() takes 1 positional argument but 3 were given\n",
    "```\n",
    "\n",
    "\n",
    "### Why we disable it\n",
    "* Privacy: we don't want our usage patterns sent anywhere.\n",
    "\n",
    "* Noise: If the telemetry hook is broken (as in this case), it pollutes our console with errors.\n",
    "\n",
    "* On-premises use: In our environment without internet access, telemetry can’t actually send data and just produces errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Chroma's local DB works\n",
    "\n",
    "When we do this:\n",
    "\n",
    "```python\n",
    "from langhchain_chroma import Chroma\n",
    "db = Chroma(persist_directory='/path/to/db', embedding_function=embedder)\n",
    "```\n",
    "\n",
    "here's what happens under the hood:\n",
    "\n",
    "1. The langchain_chroma wrapper talks to the chromadb Python package, which in turn uses a Rust library compiled into your Python process (no separate server process by default).\n",
    "2. That Rust code implements both the on‐disk storage layer and the vector search algorithms.\n",
    "3. A set of files appears in persist_directory—by default a SQLite (or DuckDB) database plus some binary files for the index.\n",
    "4. One file holds your documents and metadata (often as JSON blobs or tables), another holds the embeddings and the nearest‐neighbor index (e.g. an HNSW graph).\n",
    "5. When you first call Chroma(...), it either:\n",
    "    * Creates a new “collection” by initializing empty tables and index files, or\n",
    "    * Loads an existing collection by opening those files and preparing the in-memory structures (like loading the HNSW graph into RAM).\n",
    "\n",
    "#### Additional Notes\n",
    "* Everything lives on your local disk and memory. Queries (e.g. similarity_search) go directly against the Rust library in your process, which:\n",
    "    1. Embeds your query via your embedding_function.\n",
    "    2. Runs a nearest-neighbor search in the HNSW (or flat) index.\n",
    "    3. Retrieves the top-k document IDs and then returns the corresponding text + metadata from the on-disk tables.\n",
    "* Persistence on demand\n",
    "    * When you call db._client.persist() (or the older db.persist()), it flushes any new embeddings, documents, and index updates from memory back to those files.\n",
    "    * On your next program run, Chroma(...) simply reopens those files and you get instant search without recomputing embeddings.\n",
    "\n",
    "<u>Relevant documentation</u>:\n",
    "\n",
    "* Suppressing warnings/logging\n",
    "\n",
    "1. https://pypdf.readthedocs.io/en/stable/user/suppress-warnings.html\n",
    "2. https://docs.python.org/3/library/warnings.html\n",
    "\n",
    "* Local Chromadb client\n",
    "\n",
    "1. https://docs.trychroma.com/docs/run-chroma/persistent-client\n",
    "2. https://python.langchain.com/docs/integrations/vectorstores/chroma/\n",
    "\n",
    "* Recursive text splitting\n",
    "1. https://python.langchain.com/docs/concepts/text_splitters/\n",
    "2. https://python.langchain.com/docs/how_to/recursive_text_splitter/\n",
    "\n",
    "* PyPDF Loader\n",
    "1. https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/\n",
    "2. https://python.langchain.com/docs/how_to/document_loader_pdf/\n",
    "\n",
    "* Sim search/retrieval\n",
    "1. https://python.langchain.com/docs/how_to/vectorstore_retriever/\n",
    "2. https://python.langchain.com/docs/tutorials/retrievers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# (optional) hide any pypdf UserWarnings too\n",
    "logging.getLogger(\"pypdf\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pypdf\")\n",
    "\n",
    "# any Sentence-Transformers model works; mini-LM is light & decent\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    settings=Settings(anonymized_telemetry=False) \n",
    ")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "persist_dir = Path(\"/home/hice1/dharden7/deepsearch/langchain/chroma_db\")\n",
    "persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "db = Chroma(\n",
    "    persist_directory=str(persist_dir),\n",
    "    embedding_function=embedder,\n",
    ")\n",
    "\n",
    "if len(db.get(limit=0)) == 0:\n",
    "    print('DB is empty. Ingesting documents...\\n')\n",
    "\n",
    "    texts, metadata = [], []\n",
    "    \n",
    "    for pdf_path in Path(\"/home/hice1/dharden7/deepsearch/langchain/papers\").glob(\"*.pdf\"):\n",
    "        pages = PyPDFLoader(str(pdf_path)).load_and_split(text_splitter=splitter)\n",
    "        texts.extend([page.page_content for page in pages])\n",
    "        metadata.extend([page.metadata for page in pages])\n",
    "\n",
    "    db.add_texts(texts=texts, metadatas=metadata)\n",
    "    db._client.persist()\n",
    "else:\n",
    "    print('Loaded db from disk.')\n",
    "\n",
    "# print total chunks in db - DEBUG\n",
    "all_data = db.get(include=[\"documents\", \"metadatas\", \"embeddings\"])\n",
    "print(f\"Num stored chunks: {len(all_data['documents'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# testing the retriever\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "query= \"Give me a list of everyone on the thesis committee from the paper titled 'COLLISION INDUCED SELF ORGANIZATION IN SHAPE CHANGING ROBOTS'.\"\n",
    "results = db.similarity_search(query, k=3)\n",
    "\n",
    "if 1:\n",
    "    for i, doc in enumerate(results, start=1):\n",
    "        print(f\"\\nHit #{i}\")\n",
    "        print(doc.page_content[:500], \"...\")     \n",
    "        print(\"Metadata:\", doc.metadata)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple ReAct agent\n",
    "\n",
    "\n",
    "\n",
    "<u>Relevant documentation</u>:\n",
    "\n",
    "* Creating ReAct agent\n",
    "\n",
    "1. https://python.langchain.com/docs/tutorials/agents/\n",
    "2. https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html\n",
    "\n",
    "* Agent Orchestration (AgentExecutor)\n",
    "\n",
    "1. https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html\n",
    "\n",
    "* Prompting: Hub + PromptTemplate\n",
    "\n",
    "1. https://smith.langchain.com/hub/hwchase17/react\n",
    "2. https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html\n",
    "\n",
    "* Custom agent output parser\n",
    "\n",
    "1. https://python.langchain.com/docs/how_to/output_parser_custom/\n",
    "2. https://api.python.langchain.com/en/latest/exceptions/langchain_core.exceptions.OutputParserException.html\n",
    "\n",
    "* Creating/using tools\n",
    "\n",
    "1. https://python.langchain.com/docs/concepts/tools/\n",
    "2. https://python.langchain.com/api_reference/core/tools.html\n",
    "3. Retriever tool - https://python.langchain.com/api_reference/core/tools/langchain_core.tools.retriever.create_retriever_tool.html\n",
    "\n",
    "* Using regex for parsing\n",
    "\n",
    "1. https://docs.python.org/3/library/re.html\n",
    "2. https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain import hub  \n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ReAct parser override\n",
    "import re\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.agents.agent import AgentOutputParser\n",
    "from langchain.agents.react.output_parser import ReActOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "class QwenReActParser(AgentOutputParser):\n",
    "    # \"Action:\"\" / \"Action Input:\"\" block's\n",
    "    # \\s*(\\w+) -> skip spaces, capture the tool name (e.g. pdf_search) in group 1\n",
    "    # \\s*[\\n\\r]+ -> skip any trailing spaces, then at least one newline\n",
    "\n",
    "    _act_re = re.compile(\n",
    "        r\"Action:\\s*(\\w+)\\s*[\\n\\r]+Action Input:\\s*(.*?)\\s*$\",\n",
    "        re.I | re.S,\n",
    "    )\n",
    "\n",
    "    # \"Final Answer:\" block\n",
    "    # non-greedy capture (.*?) between the two markers\n",
    "    # \\s* allows for newline / spaces before the [END ANSWER]\n",
    "    _final_re = re.compile(\n",
    "        r\"Final Answer:\\s*(.*?)\\s*\\[END ANSWER\\]\",\n",
    "        re.I | re.S,\n",
    "    )\n",
    "\n",
    "    # if [END ANSWER] is missing grab rest of text\n",
    "    _fallback_re = re.compile(\n",
    "        r\"Final Answer:\\s*(.*)\",\n",
    "        re.I | re.S,\n",
    "    )\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        # block with END marker\n",
    "        m = self._final_re.search(text)\n",
    "        if m:\n",
    "            answer = m.group(1).strip()\n",
    "            return AgentFinish({\"output\": answer}, log=text)\n",
    "\n",
    "        # \"Final Answer:\" exists but no [END ANSWER] marker\n",
    "        m_fb = self._fallback_re.search(text)\n",
    "        if m_fb:\n",
    "            answer = m_fb.group(1).splitlines()[0].strip()\n",
    "            return AgentFinish({\"output\": answer}, log=text)\n",
    "\n",
    "        #  \"Action:\"\" / \"Action Input:\"\" block's\n",
    "        # i.e group 0: 'Action: pdf_search\\nAction Input: \"COLLISION INDUCED SELF ORGANIZATION IN SHAPE CHANGING ROBOTS\"'\n",
    "        # group(1): 'pdf_search'\n",
    "        # group(2): '\"COLLISION INDUCED SELF ORGANIZATION IN SHAPE CHANGING ROBOTS\"'\n",
    "        m_act = self._act_re.search(text)\n",
    "        if m_act:\n",
    "            return AgentAction(\n",
    "                tool=m_act.group(1).strip(),\n",
    "                tool_input=m_act.group(2).strip().strip('\"'),\n",
    "                log=text,\n",
    "            )\n",
    "\n",
    "        raise OutputParserException(f\"Could not parse LLM output:\\n{text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "# when the agent doesn't follow the expected Thought - Action - Action Input - Final Answer format\n",
    "# handle OutputParserException's by reminding it how to format its answers\n",
    "def fix_format(error: Exception) -> str:\n",
    "    return (\n",
    "        \"FORMAT ERROR: \"           \n",
    "        \"When you want to use a tool, reply ONLY with:\\n\"\n",
    "        \"Thought: <your thought>\\n\"\n",
    "        \"Action: <tool name>\\n\"\n",
    "        \"Action Input: <input>\\n\"\n",
    "        \"Do NOT put Final Answer in that same response.\\n\"\n",
    "        \"When you are completely finished and need no more tools, \"\n",
    "        \"reply ONLY with:\\n\"\n",
    "        \"Thought: <your thought>\\n\"\n",
    "        \"Final Answer: <answer>\\n\"\n",
    "    )\n",
    "\n",
    "extra_rule = (\n",
    "    \"\\nIMPORTANT:\\n\"\n",
    "    \"After you emit a line that begins with 'Final Answer:' **You must output nothing else.** \"\n",
    "    \"Do NOT add any further Thought, Action, or Observation.\"\n",
    "    # \"NEVER repeat the rules given to you in your Final Answer.\"\n",
    "    \"Keep your 'Final Answer' as concise as possible while providing all relevant details and explanations.\"\n",
    "    \"Once you are satisfied with your final answer, write '\\n[END ANSWER]\\n[ANSWER END]'.\"\n",
    ")\n",
    "\n",
    "# create base prompt for the ai agent\n",
    "base_prompt: PromptTemplate = hub.pull(\"hwchase17/react\")\n",
    "strict_prompt = PromptTemplate(\n",
    "    input_variables=base_prompt.input_variables,\n",
    "    template=base_prompt.template + \"\\nNEVER write “Final Answer:” until AFTER you have read an Observation.\" + extra_rule,\n",
    ")\n",
    "\n",
    "# setup retrieval tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    # default knn search\n",
    "    retriever = db.as_retriever(),\n",
    "    name = \"pdf_search\",\n",
    "    description = (\n",
    "        \"Searches the ingested PDF knowledge base and returns relevant passages with metadata such as source file and page number.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "def pdf_search_string(query: str, k: int = 3) -> str:\n",
    "    docs = db.similarity_search(query, k=k)\n",
    "    if not docs:\n",
    "        return \"NO_MATCH\"\n",
    "\n",
    "    passages = []\n",
    "    for d in docs:\n",
    "        src  = d.metadata.get(\"source\", \"unknown\").split(\"/\")[-1]\n",
    "        page = d.metadata.get(\"page_label\") or d.metadata.get(\"page\")\n",
    "        passages.append(\n",
    "            f\"[SOURCE: {src} | page {page}]\\n\"\n",
    "            # get first 500 char per hit\n",
    "            f\"{d.page_content.strip()[:500]}...\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n-\\n\\n\".join(passages)\n",
    "\n",
    "# define the search tool\n",
    "pdf_search_tool = Tool(\n",
    "    name=\"pdf_search\",\n",
    "    func=pdf_search_string,\n",
    "    description=(\n",
    "        \"Searches the ingested PDF knowledge base and returns up to 3 passages \"\n",
    "        \"that mention the query, each prefixed with source file and page.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# create our react agent\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    [pdf_search_tool],\n",
    "    prompt=strict_prompt,\n",
    "    output_parser=QwenReActParser()\n",
    ")\n",
    "\n",
    "# create an agent executor that orchestrates the prompting + tool calling pipeline\n",
    "agent_exec = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=[pdf_search_tool],\n",
    "    verbose=False,\n",
    "    handle_parsing_errors=fix_format,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# let's test the agent's retrieval tool\n",
    "# question = \"Give me a list of everyone on the thesis committee from the paper titled 'COLLISION INDUCED SELF ORGANIZATION IN SHAPE CHANGING ROBOTS'.\"\n",
    "question = \"Give me a list of everyone who approved the paper whose title goes as 'Development of Novel Platforms for Homogeneous and Heterogeneous Catalysis'\"\n",
    "\n",
    "if 1:\n",
    "    result = agent_exec.invoke({\"input\": question})\n",
    "    print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversing with our agent as a chatbot with memory\n",
    "\n",
    "<u>Relevant documentation</u>:\n",
    "\n",
    "* Conversation with memory\n",
    "\n",
    "1. https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html\n",
    "2. https://python.langchain.com/docs/versions/migrating_memory/conversation_buffer_memory/\n",
    "3. https://medium.com/%40danushidk507/memory-in-langchain-iii-f0a226f5eb65\n",
    "\n",
    "* Text coloring w/ANSI\n",
    "\n",
    "1. https://jakob-bagterp.github.io/colorist-for-python/ansi-escape-codes/standard-16-colors/\n",
    "2. https://python-reference.readthedocs.io/en/latest/docs/file/flush.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "import sys, time\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "# Prevent messages like \"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    # The key name under which the memory object will return the stored conversation when a chain/agent loads memory variables\n",
    "    memory_key=\"chat_history\",\n",
    "\n",
    "    '''\n",
    "    Tells the memory which field in the chain’s outputs contains the model’s reply that should be written back into memory after each run\n",
    "    For instance the chain could store the models outputs under \"Answer\" or \"Result\"\n",
    "    '''\n",
    "    output_key=\"output\",\n",
    "\n",
    "    '''\n",
    "    Controls the format in which past conversation is returned\n",
    "    True: You get a list of structured HumanMessage / AIMessage objects (recommended for chat models)\n",
    "    False: You get a single concatenated string buffer, which can behave unpredictably with chat-oriented LLMs\n",
    "    '''\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "chatbot = AgentExecutor(\n",
    "    agent = agent,\n",
    "    tools = [pdf_search_tool],\n",
    "    memory = memory,\n",
    "    verbose = False,               \n",
    "    return_intermediate_steps = False \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# ANSI escape codes for coloring terminal text\n",
    "BLUE   = \"\\033[94m\"\n",
    "GREEN  = \"\\033[92m\"\n",
    "DEF  = \"\\033[0m\"\n",
    "\n",
    "while True:\n",
    "    user_msg = input(f\"{BLUE}You:{DEF} \")\n",
    "    if user_msg.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    # show the user’s message right away\n",
    "    print(f\"{BLUE}You:{DEF} {user_msg}\", flush=True)\n",
    "\n",
    "    response = chatbot.invoke({\"input\": user_msg})\n",
    "    agent_reply = response[\"output\"]\n",
    "\n",
    "    print(f\"{GREEN}Agent:{DEF} {agent_reply}\\n\", flush=True)\n",
    "\n",
    "    '''\n",
    "    Give Jupyter a moment to push the buffer to UI. Handy in interactive chat loops so user sees \n",
    "    streaming output without waiting for buffer flush on newline/exit.\n",
    "    '''\n",
    "    sys.stdout.flush()\n",
    "    time.sleep(0.05)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
