{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5531e45",
   "metadata": {},
   "source": [
    "# RAG-Based Chatbot Tutorial: Complete Code Implementation\n",
    "\n",
    "This comprehensive tutorial demonstrates how to build a RAG (Retrieval-Augmented Generation) chatbot using the KG-RAG dataset and PACE ICE's compute resources. Unlike the conceptual tutorial, this version provides complete, executable code examples for every step.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Data Preparation](#data-preparation)\n",
    "3. [Text Processing and Chunking](#text-processing-and-chunking)\n",
    "4. [Embedding Generation](#embedding-generation)\n",
    "5. [RAG Implementation](#rag-implementation)\n",
    "6. [Model Management](#model-management)\n",
    "7. [Evaluation System](#evaluation-system)\n",
    "8. [Complete Workflow](#complete-workflow)\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "### Prerequisites\n",
    "- Python: basic knowledge (for loops, method calls, generators, etc)\n",
    "- Bash: basic knowledge (for loops, variable substitution, etc)\n",
    "- Access to [PACE ICE's instance of Open OnDemand](https://ondemand-ice.pace.gatech.edu)\n",
    "  - If off-campus, use the GT VPN (or in-browser VPN) to connect\n",
    "\n",
    "### Step 1: Get Started with ICE\n",
    "\n",
    "We are going to be running LLMs (large language models) on ICE. The easiest way to get started with LLMs on ICE is to go to Open OnDemand, click \"Interactive Apps\" on the top navbar, and then \"Ollama + Jupyter (Beta).\"\n",
    "\n",
    "![Interactive Apps dropdown showing Ollama option](https://github.com/user-attachments/assets/8d4b9deb-90f4-48f6-98b6-c1e6b163cbd0)\n",
    "\n",
    "#### Configure Ollama Settings\n",
    "\n",
    "1. **Ollama Models Directory**: Select \"Temporary directory\" as your Ollama models directory. This directory is where the LLMs will be downloaded to. Since \"PACE shared models\" does not allow downloading additional models, we will use \"Temporary directory\" and download the models ourselves.\n",
    "\n",
    "<img src='https://github.com/user-attachments/assets/1397c880-1892-4987-b718-eb6d1c1eeb48' alt='dropdown for choosing Ollama models directory' width='512'/>\n",
    "\n",
    "2. **Node Type**: For the node type, select \"NVIDIA GPU (first avail).\" This will ensure you get a GPU of some sort, while making sure you won't wait too long for a specific GPU to free up.\n",
    "\n",
    "3. **Other Settings**: The default values can be used for everything else.\n",
    "\n",
    "#### Launch Your Environment\n",
    "\n",
    "Once you click submit, you should see a card like the following:\n",
    "\n",
    "![Card showing queued job status](https://github.com/user-attachments/assets/9503a326-2f65-4dea-b04f-317a125c4d01)\n",
    "\n",
    "This card will become green once the environment is ready:\n",
    "\n",
    "![Card showing running job status with Connect to Jupyter button](https://github.com/user-attachments/assets/79c02363-b076-402c-851f-aa118504e6ce)\n",
    "\n",
    "Click \"Connect to Jupyter\" to open a new tab with your new environment.\n",
    "\n",
    "### Step 2: Setting up your Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60563a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aebe44",
   "metadata": {},
   "source": [
    "```\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# run outside of a notebook\n",
    "\n",
    "python -m venv .venv\n",
    ".venv/bin/pip install ipykernel\n",
    ".venv/bin/ipykernel install --user --name=ospo-ai-agent\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25de369",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81eeae",
   "metadata": {},
   "source": [
    "### Package Details\n",
    "\n",
    "Each dependency enables specific capabilities in our RAG system:\n",
    "\n",
    "#### Ollama\n",
    "Interface with local LLM models running on ICE infrastructure. This enables running powerful language models (like Llama, Mistral) locally without API costs or internet dependency. Essential for both text generation and embeddings in our RAG system.\n",
    "\n",
    "#### NLTK (Natural Language Toolkit)\n",
    "Sentence tokenization and text preprocessing. Breaks documents into meaningful sentences for semantic chunking. Proper sentence boundaries are crucial for maintaining context when splitting text - better than naive character-based splitting that might break mid-sentence.\n",
    "\n",
    "#### NumPy\n",
    "Mathematical operations on embedding vectors. Enables efficient cosine similarity calculations between embeddings. Critical for semantic chunking (comparing sentence similarities) and vector database operations. Much faster than pure Python for numerical computations.\n",
    "\n",
    "#### ChromaDB\n",
    "Vector database for storing and retrieving document embeddings. Provides fast semantic search capabilities - instead of keyword matching, finds documents that are conceptually similar to queries. Handles the complex vector similarity math automatically and scales to large document collections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55163c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ollama\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Verify installations\n",
    "try:\n",
    "    import chromadb\n",
    "    import numpy as np\n",
    "    print(\"✅ All imports successful!\")\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "    print(\"✅ Environment setup complete!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c99e2d",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Downloading KG-RAG Dataset\n",
    "\n",
    "The dataset contains SEC 10-Q filings with corresponding questions and answers for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download KG-RAG dataset using IPython shell commands\n",
    "!bash get-pdfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10830611",
   "metadata": {},
   "source": [
    "## Text Summarization\n",
    "\n",
    "### Why Summarization Comes First\n",
    "\n",
    "Before chunking text, we often need to summarize it because:\n",
    "\n",
    "1. **Remove Template Text**: Documents often contain headers, legal notices, and formatting that don't add value\n",
    "2. **Prevent Hallucination**: Based on anecdotal evidence, LLMs are more likely to hallucinate with unnecessary information\n",
    "\n",
    "### Intelligent Document Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c35390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document summarization with relevance filtering\n",
    "import itertools\n",
    "import json\n",
    "import ollama\n",
    "\n",
    "def summarize_document(text_path: str, summary_path: str, \n",
    "                      chunk_size=8000, model=\"llama3.2:3b\"):\n",
    "    \"\"\"\n",
    "    Summarize a document by processing it in chunks and filtering relevant content.\n",
    "    \n",
    "    Args:\n",
    "        text_path: Path to input text file\n",
    "        summary_path: Path to save summary\n",
    "        chunk_size: Size of text chunks for processing\n",
    "        model: Model to use for summarization\n",
    "    \"\"\"\n",
    "    with open(text_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Split into manageable chunks\n",
    "    chunks = list(itertools.batched(text, chunk_size))\n",
    "    full_summary = \"\"\n",
    "    \n",
    "    print(f\"Processing {len(chunks)} chunks...\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_text = ''.join(chunk)\n",
    "        print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
    "        \n",
    "        # Generate summary\n",
    "        summary_response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=f\"\"\"Make a concise, informative summary of the following text from {text_path}:\n",
    "\n",
    "{chunk_text}\n",
    "\n",
    "Requirements:\n",
    "- Use Markdown formatting\n",
    "- Focus on key facts and figures\n",
    "- Be self-contained and clear\n",
    "- If there's no important information, return an empty summary\n",
    "\n",
    "Summary:\"\"\"\n",
    "        )\n",
    "        \n",
    "        summary = summary_response[\"response\"]\n",
    "        print(f\"Generated summary: {summary[:100]}...\")\n",
    "        \n",
    "        # Check if summary contains useful information\n",
    "        relevance_response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=f\"\"\"Does the following summary contain useful, specific information (not just formatting or generic statements)?\n",
    "\n",
    "Summary: {summary}\n",
    "\n",
    "Respond with JSON format: {{\"relevant\": true}} or {{\"relevant\": false}}\"\"\"\n",
    "        )\n",
    "        \n",
    "        relevance_text = relevance_response[\"response\"]\n",
    "        print(f\"Relevance check: {relevance_text}\")\n",
    "        \n",
    "        # Parse relevance (simple heuristic)\n",
    "        is_relevant = (\"true\" in relevance_text.lower() and \n",
    "                      \"false\" not in relevance_text.lower())\n",
    "        \n",
    "        if is_relevant:\n",
    "            full_summary += summary + \"\\n\\n\"\n",
    "            print(\"✓ Summary added to final document\")\n",
    "        else:\n",
    "            print(\"✗ Summary skipped (not relevant)\")\n",
    "    \n",
    "    # Save final summary\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(full_summary.strip())\n",
    "    \n",
    "    print(f\"Complete summary saved to {summary_path}\")\n",
    "    return full_summary\n",
    "\n",
    "# Example usage\n",
    "def demo_summarization():\n",
    "    # Create sample document\n",
    "    sample_doc = \"\"\"\n",
    "    FINANCIAL PERFORMANCE OVERVIEW\n",
    "    \n",
    "    Revenue for Q3 2024 reached $89.5 billion, representing a 6% increase compared to Q3 2023.\n",
    "    iPhone revenue was $46.2 billion, up from $43.8 billion in the prior year quarter.\n",
    "    Services revenue grew to $22.3 billion, a 12% increase year-over-year.\n",
    "    \n",
    "    OPERATING EXPENSES\n",
    "    \n",
    "    Research and development expenses were $7.8 billion for the quarter.\n",
    "    Sales and marketing expenses totaled $1.2 billion.\n",
    "    General and administrative expenses were $0.6 billion.\n",
    "    \n",
    "    GEOGRAPHIC BREAKDOWN\n",
    "    \n",
    "    Americas revenue: $37.2 billion\n",
    "    Europe revenue: $22.5 billion  \n",
    "    Greater China revenue: $15.1 billion\n",
    "    Japan revenue: $5.9 billion\n",
    "    Rest of Asia Pacific revenue: $8.8 billion\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('sample_financial.txt', 'w') as f:\n",
    "        f.write(sample_doc)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = summarize_document('sample_financial.txt', 'sample_summary.txt')\n",
    "    print(\"\\n--- FINAL SUMMARY ---\")\n",
    "    print(summary)\n",
    "\n",
    "# Uncomment to run demo\n",
    "# demo_summarization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc834d4b",
   "metadata": {},
   "source": [
    "## Text Processing and Chunking\n",
    "\n",
    "### Why Text Chunking is Important\n",
    "\n",
    "Text chunking is a critical step in RAG systems because:\n",
    "\n",
    "1. **LLM Context Limits**: Large language models have maximum context lengths (token limits). Even with large context windows, performance often degrades with very long inputs.\n",
    "\n",
    "2. **Retrieval Precision**: When you search a vector database, you want to retrieve the most relevant pieces of information, not entire documents that may contain irrelevant sections.\n",
    "\n",
    "3. **Quality Over Quantity**: Based on anecdotal evidence, an LLM is more likely to hallucinate if given a *lot* of unnecessary information. Therefore, to make a good RAG chatbot, we want to give the LLM the minimum amount of information that still results in a good answer.\n",
    "\n",
    "### How Chunking Works\n",
    "\n",
    "There are several approaches to chunking text:\n",
    "\n",
    "- **Fixed-length chunking**: Split text every N characters or words (simple but breaks context)\n",
    "- **Sentence-based chunking**: Split at sentence boundaries (better context preservation)\n",
    "- **Semantic chunking**: Split based on topic changes using embedding similarity (optimal context)\n",
    "\n",
    "### Semantic Chunking Implementation\n",
    "\n",
    "Semantic chunking uses embedding similarity to determine natural breakpoints in text, creating more coherent chunks than simple length-based splitting. This approach identifies topic changes by measuring the similarity between consecutive sentences.\n",
    "\n",
    "**Note**: This chunking process is typically applied to the summarized text from the previous step, ensuring we chunk clean, relevant content rather than raw documents with formatting artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic chunking implementation\n",
    "import sys\n",
    "import ollama\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compare_vec(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    mag_a = np.linalg.norm(a)\n",
    "    mag_b = np.linalg.norm(b)\n",
    "    return dot_product / mag_a / mag_b\n",
    "\n",
    "def semantic_chunk(text: str, embed_model='nomic-embed-text', percentile=0.95):\n",
    "    \"\"\"\n",
    "    Split text into semantically coherent chunks using embedding similarity.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        embed_model: Model to use for embeddings\n",
    "        percentile: Similarity threshold percentile for splitting\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks (each chunk is a list of sentences)\n",
    "    \"\"\"\n",
    "    # Split text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    print(f\"Processing {len(sentences)} sentences...\", file=sys.stderr)\n",
    "    \n",
    "    # Generate embeddings for each sentence\n",
    "    embeddings = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Embedding sentence {i+1}/{len(sentences)}\", file=sys.stderr)\n",
    "        embedding = ollama.embed(embed_model, sentence)['embeddings'][0]\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    if len(embeddings) < 2:\n",
    "        return [sentences]\n",
    "    \n",
    "    # Calculate similarity between consecutive sentences\n",
    "    print(f\"Comparing {len(embeddings)} embeddings...\", file=sys.stderr)\n",
    "    deltas = [compare_vec(*x) for x in zip(embeddings, embeddings[1:])]\n",
    "    \n",
    "    # Determine splitting threshold\n",
    "    deltas_sorted = sorted(deltas)\n",
    "    threshold = deltas_sorted[int(len(deltas_sorted) * percentile)]\n",
    "    print(f\"Similarity threshold ({percentile*100:.2f} percentile): {threshold:.4f}\", file=sys.stderr)\n",
    "    \n",
    "    # Split at low similarity points\n",
    "    chunks = []\n",
    "    last_split = 0\n",
    "    for i, delta in enumerate(deltas):\n",
    "        if delta <= threshold:  # Low similarity = good split point\n",
    "            chunks.append(sentences[last_split:i+1])\n",
    "            last_split = i + 1\n",
    "            print(f\"Split after sentence {i+1} (similarity: {delta:.4f})\")\n",
    "    \n",
    "    # Add remaining sentences\n",
    "    if last_split < len(sentences):\n",
    "        chunks.append(sentences[last_split:])\n",
    "    \n",
    "    print(f\"Created {len(chunks)} semantic chunks\", file=sys.stderr)\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "def demo_semantic_chunking():\n",
    "    sample_text = \"\"\"\n",
    "    Apple Inc. reported strong quarterly results. Revenue increased by 15% year-over-year.\n",
    "    The iPhone segment saw particularly strong growth. Sales in China recovered significantly.\n",
    "    \n",
    "    In other news, the company announced a new data center initiative. \n",
    "    This represents a major shift in infrastructure strategy.\n",
    "    The project will require substantial capital investment over the next three years.\n",
    "    \n",
    "    Looking ahead, management remains optimistic about growth prospects.\n",
    "    However, they cautioned about potential supply chain disruptions.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = semantic_chunk(sample_text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(' '.join(chunk))\n",
    "\n",
    "# Uncomment to run demo\n",
    "# demo_semantic_chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25585b02",
   "metadata": {},
   "source": [
    "## Embedding Generation\n",
    "\n",
    "### What are Embeddings and Why Use Them?\n",
    "\n",
    "Embeddings are numerical vector representations of text that capture semantic meaning. In a RAG system, embeddings enable:\n",
    "\n",
    "1. **Semantic Search**: Instead of exact keyword matching, you can find documents that are conceptually related to your query\n",
    "2. **Vector Database Storage**: Embeddings allow fast similarity searches using mathematical operations like cosine similarity\n",
    "3. **Context Understanding**: Similar concepts have similar embeddings, even if they use different words\n",
    "\n",
    "### How Vector Databases Work\n",
    "\n",
    "A vector database stores and retrieves text by its embedding vector, generated by an embedding model like `nomic-embed-text`. The simplified workflow (in pseudocode) is:\n",
    "\n",
    "```\n",
    "embed(text: str) -> List[float]                         # Convert text to numbers\n",
    "vector_db_store(id: str, vector: List[float])           # Store with identifier\n",
    "vector_db_query(vector: List[float]) -> List[Document]  # Find similar\n",
    "```\n",
    "\n",
    "For example, storing information about pies (in pseudocode)\n",
    "```\n",
    "vector_db_store('pumpkin_pie', embed('pumpkin pie'))\n",
    "vector_db_store('apple_pie', embed('apple pie'))\n",
    "vector_db_store('elderberry_pie', embed('elderberry pie'))\n",
    "```\n",
    "\n",
    "When a user asks about \"pumpkins\", running `vector_db_query(embed('pumpkins'))` would return `'pumpkin_pie'` as the top result because the embeddings are mathematically similar.\n",
    "\n",
    "### Simple Embedding Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic embedding generation\n",
    "import json\n",
    "import ollama\n",
    "\n",
    "def generate_embedding(text_path: str, embedding_path: str, model='nomic-embed-text'):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a text file.\n",
    "    \n",
    "    Args:\n",
    "        text_path: Path to input text file\n",
    "        embedding_path: Path to save embedding JSON\n",
    "        model: Embedding model to use\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(text_path, 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        print(f\"Generating embedding for {text_path}...\")\n",
    "        resp = ollama.embed(model, text)\n",
    "        \n",
    "        with open(embedding_path, 'w') as f:\n",
    "            json.dump(resp[\"embeddings\"], f)\n",
    "        \n",
    "        print(f\"Embedded {text_path} -> {embedding_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding {text_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "def demo_embedding():\n",
    "    # Create sample text file\n",
    "    sample_text = \"Apple Inc. is a technology company that designs consumer electronics.\"\n",
    "    with open('sample.txt', 'w') as f:\n",
    "        f.write(sample_text)\n",
    "    \n",
    "    # Generate embedding\n",
    "    success = generate_embedding('sample.txt', 'sample_embedding.json')\n",
    "    \n",
    "    if success:\n",
    "        # Load and examine embedding\n",
    "        with open('sample_embedding.json', 'r') as f:\n",
    "            embedding = json.load(f)\n",
    "        print(f\"Embedding dimensions: {len(embedding[0])}\")\n",
    "        print(f\"First 5 values: {embedding[0][:5]}\")\n",
    "\n",
    "# Uncomment to run demo\n",
    "# demo_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2500ea4",
   "metadata": {},
   "source": [
    "## RAG Implementation\n",
    "\n",
    "### Simple RAG System\n",
    "\n",
    "Using global variables and functions for clarity in notebook environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG implementation with global variables\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "# Global configuration\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "GENERATE_MODEL = \"llama3.3\"\n",
    "\n",
    "# Global database connection\n",
    "client = chromadb.Client()\n",
    "collection = None\n",
    "\n",
    "def load_documents_from_embeddings(embeddings_path: str):\n",
    "    \"\"\"Load documents from pre-computed embeddings directory\"\"\"\n",
    "    global collection\n",
    "    collection = client.create_collection(name=\"docs\")\n",
    "    \n",
    "    files = os.listdir(embeddings_path)\n",
    "    files = [name for name in files if name[0] != '.' and not name.endswith(\".marker\")]\n",
    "    \n",
    "    n_loaded = 0\n",
    "    last_print = time.time()\n",
    "    \n",
    "    for name in files:\n",
    "        now = time.time()\n",
    "        if now - last_print > 1:\n",
    "            print(f\"Loaded {n_loaded}/{len(files)} documents\")\n",
    "            last_print = now\n",
    "        \n",
    "        try:\n",
    "            with open(os.path.join(embeddings_path, name), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                collection.add(\n",
    "                    ids=[name],\n",
    "                    embeddings=data['vector'][0],\n",
    "                    documents=data['chunk_text'],\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        n_loaded += 1\n",
    "    \n",
    "    print(f\"Successfully loaded {n_loaded} documents\")\n",
    "    return collection\n",
    "\n",
    "def load_documents_from_text(embeddings_path: str, text_path: str):\n",
    "    \"\"\"Load documents from text files with corresponding embeddings\"\"\"\n",
    "    global collection\n",
    "    collection = client.create_collection(name=\"docs\")\n",
    "    \n",
    "    files = os.listdir(text_path)\n",
    "    files = [name for name in files if name[0] != '.']\n",
    "    \n",
    "    n_loaded = 0\n",
    "    for name in files:\n",
    "        basename = os.path.splitext(name)[0]\n",
    "        \n",
    "        if (n_loaded / len(files) * 100) % 10 == 0:\n",
    "            print(f\"Loaded {n_loaded}/{len(files)} documents\")\n",
    "        \n",
    "        try:\n",
    "            # Load text\n",
    "            with open(os.path.join(text_path, basename + \".txt\"), 'r') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Load corresponding embedding\n",
    "            with open(os.path.join(embeddings_path, basename + \".json\"), 'r') as f:\n",
    "                embedding = json.load(f)\n",
    "            \n",
    "            collection.add(\n",
    "                ids=[name],\n",
    "                embeddings=embedding,\n",
    "                documents=text,\n",
    "            )\n",
    "            n_loaded += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully loaded {n_loaded} documents\")\n",
    "    return collection\n",
    "\n",
    "def generate_retrieval_query(user_query: str):\n",
    "    \"\"\"Generate optimized search query from user input\"\"\"\n",
    "    response = ollama.generate(\n",
    "        GENERATE_MODEL,\n",
    "        prompt=f\"\"\"You are a search agent. Transform this user query into optimal keywords for document retrieval.\n",
    "\n",
    "User Query: {user_query}\n",
    "\n",
    "Return ONLY the search keywords, nothing else - no explanation or additional text.\"\"\"\n",
    "    )\n",
    "    return response[\"response\"]\n",
    "\n",
    "def retrieve_documents(query: str, n_results=8):\n",
    "    \"\"\"Retrieve relevant documents using vector similarity\"\"\"\n",
    "    if not collection:\n",
    "        raise RuntimeError(\"No documents loaded. Call load_documents_* first.\")\n",
    "    \n",
    "    # Generate retrieval query\n",
    "    retrieval_query = generate_retrieval_query(query)\n",
    "    print(f\"Search query: {retrieval_query}\")\n",
    "    \n",
    "    # Get query embedding\n",
    "    embedding_response = ollama.embed(EMBED_MODEL, retrieval_query)\n",
    "    \n",
    "    # Query vector database\n",
    "    results = collection.query(\n",
    "        query_embeddings=embedding_response[\"embeddings\"],\n",
    "        n_results=n_results,\n",
    "    )\n",
    "    \n",
    "    if len(results['ids'][0]) == 0:\n",
    "        raise RuntimeError(\"No documents retrieved\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def filter_relevant_documents(query: str, results):\n",
    "    \"\"\"Filter documents based on relevance to query\"\"\"\n",
    "    texts = results['documents'][0]\n",
    "    ids = results['ids'][0]\n",
    "    \n",
    "    print(\"Filtering documents for relevance:\")\n",
    "    relevant_docs = []\n",
    "    \n",
    "    for i, (doc_id, text) in enumerate(zip(ids, texts)):\n",
    "        # Check if document contributes to answering the query\n",
    "        relevance_response = ollama.generate(\n",
    "            GENERATE_MODEL,\n",
    "            prompt=f\"\"\"Analyze if this document helps answer the user's question.\n",
    "\n",
    "User Question: {query}\n",
    "Document ID: {doc_id}\n",
    "\n",
    "Does this document contain information that significantly contributes to answering the question?\n",
    "Answer with: YES or NO\"\"\"\n",
    "        )\n",
    "        \n",
    "        if \"YES\" in relevance_response[\"response\"]:\n",
    "            relevant_docs.append(text)\n",
    "            print(f\"✓ {doc_id}\")\n",
    "        else:\n",
    "            print(f\"✗ {doc_id}\")\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "def ask_question(query: str, use_filtering=True):\n",
    "    \"\"\"Answer a question using RAG\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    results = retrieve_documents(query)\n",
    "    \n",
    "    # Get document texts\n",
    "    if use_filtering:\n",
    "        relevant_texts = filter_relevant_documents(query, results)\n",
    "    else:\n",
    "        relevant_texts = results['documents'][0]\n",
    "    \n",
    "    if not relevant_texts:\n",
    "        return \"I couldn't find relevant information to answer your question.\"\n",
    "    \n",
    "    # Generate answer using retrieved context\n",
    "    context = '\\n'.join(relevant_texts)\n",
    "    print(\"Generating answer...\")\n",
    "    \n",
    "    response = ollama.generate(\n",
    "        GENERATE_MODEL,\n",
    "        prompt=f\"\"\"Answer the user's question using the provided context. Be accurate and specific.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    \n",
    "    return response[\"response\"]\n",
    "\n",
    "def start_interactive_session():\n",
    "    \"\"\"Start interactive question-answering session\"\"\"\n",
    "    print(\"RAG System ready! Type 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\n> \")\n",
    "            \n",
    "            if query.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            answer = ask_question(query)\n",
    "            print(f\"\\n{answer}\\n\")\n",
    "            \n",
    "        except EOFError:\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Example usage\n",
    "def demo_rag_system():\n",
    "    # Create some sample documents and embeddings for demo\n",
    "    sample_docs = [\n",
    "        (\"doc1.json\", {\n",
    "            \"vector\": [[0.1, 0.2, 0.3, 0.4, 0.5]],  # Simplified embedding\n",
    "            \"chunk_text\": \"Apple Inc. reported revenue of $89.5 billion in Q3 2024, up 6% year-over-year. iPhone sales contributed $46.2 billion.\"\n",
    "        }),\n",
    "        (\"doc2.json\", {\n",
    "            \"vector\": [[0.2, 0.3, 0.4, 0.5, 0.6]],\n",
    "            \"chunk_text\": \"Operating expenses included $7.8 billion for R&D and $1.2 billion for sales and marketing.\"\n",
    "        }),\n",
    "        (\"doc3.json\", {\n",
    "            \"vector\": [[0.3, 0.4, 0.5, 0.6, 0.7]],\n",
    "            \"chunk_text\": \"Revenue breakdown by geography: Americas $37.2B, Europe $22.5B, Greater China $15.1B.\"\n",
    "        })\n",
    "    ]\n",
    "    \n",
    "    # Create sample embedding files\n",
    "    os.makedirs(\"sample_embeddings\", exist_ok=True)\n",
    "    for filename, data in sample_docs:\n",
    "        with open(f\"sample_embeddings/{filename}\", 'w') as f:\n",
    "            json.dump(data, f)\n",
    "    \n",
    "    print(\"Sample documents created!\")\n",
    "    print(\"To use RAG system:\")\n",
    "    print(\"1. Load documents: load_documents_from_embeddings('embed2')\")\n",
    "    print(\"2. Ask questions: ask_question('What was the revenue?')\")\n",
    "    print(\"3. Start interactive mode: start_interactive_session()\")\n",
    "\n",
    "# Run demo\n",
    "demo_rag_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d10ca",
   "metadata": {},
   "source": [
    "## Model Management\n",
    "\n",
    "### Automated Model Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model management system\n",
    "import ollama\n",
    "import subprocess\n",
    "\n",
    "def list_available_models():\n",
    "    \"\"\"List currently available models\"\"\"\n",
    "    try:\n",
    "        models = ollama.list()[\"models\"]\n",
    "        print(\"Currently available models:\")\n",
    "        for model in models:\n",
    "            print(f\"  - {model.model}\")\n",
    "        return [model.model for model in models]\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_models():\n",
    "    \"\"\"Download essential models for RAG system\"\"\"\n",
    "    # Essential models for RAG\n",
    "    models_to_download = [\n",
    "        'nomic-embed-text',    # Embedding model\n",
    "        'llama3.1:8b',        # Large general model\n",
    "        'llama3.2:1b',        # Small fast model\n",
    "        'llama3.2:3b',        # Medium model (good balance)\n",
    "        'llama3.3',           # Latest model\n",
    "        'mistral',            # Alternative architecture\n",
    "        'qwen2.5:0.5b',       # Very small model for testing\n",
    "        'qwen2.5:1.5b',       # Small efficient model\n",
    "    ]\n",
    "    \n",
    "    print(\"Downloading essential models...\")\n",
    "    successful_downloads = []\n",
    "    failed_downloads = []\n",
    "    \n",
    "    for model in models_to_download:\n",
    "        try:\n",
    "            print(f\"\\nDownloading {model}...\")\n",
    "            ollama.pull(model)\n",
    "            successful_downloads.append(model)\n",
    "            print(f\"✓ {model} downloaded successfully\")\n",
    "        except Exception as e:\n",
    "            failed_downloads.append((model, str(e)))\n",
    "            print(f\"✗ Failed to download {model}: {e}\")\n",
    "    \n",
    "    print(f\"\\n--- Download Summary ---\")\n",
    "    print(f\"Successful: {len(successful_downloads)}\")\n",
    "    print(f\"Failed: {len(failed_downloads)}\")\n",
    "    \n",
    "    if successful_downloads:\n",
    "        print(\"\\nSuccessfully downloaded:\")\n",
    "        for model in successful_downloads:\n",
    "            print(f\"  ✓ {model}\")\n",
    "    \n",
    "    if failed_downloads:\n",
    "        print(\"\\nFailed downloads:\")\n",
    "        for model, error in failed_downloads:\n",
    "            print(f\"  ✗ {model}: {error}\")\n",
    "\n",
    "def get_model_info(model_name: str):\n",
    "    \"\"\"Get detailed information about a model\"\"\"\n",
    "    try:\n",
    "        # Test if model is available\n",
    "        models = ollama.list()[\"models\"]\n",
    "        available_models = [m.model for m in models]\n",
    "        \n",
    "        if model_name not in available_models:\n",
    "            print(f\"Model {model_name} is not available. Available models:\")\n",
    "            for model in available_models:\n",
    "                print(f\"  - {model}\")\n",
    "            return None\n",
    "        \n",
    "        # Test model with a simple query\n",
    "        test_response = ollama.generate(\n",
    "            model_name, \n",
    "            prompt=\"Say 'Hello' if you're working correctly.\",\n",
    "            options={\"num_predict\": 10}\n",
    "        )\n",
    "        \n",
    "        print(f\"Model {model_name} is working correctly.\")\n",
    "        print(f\"Test response: {test_response['response']}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing model {model_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "def demo_model_management():\n",
    "    print(\"=== Model Management Demo ===\")\n",
    "    \n",
    "    # List current models\n",
    "    current_models = list_available_models()\n",
    "    \n",
    "    # Download models (uncomment to actually download)\n",
    "    print(\"\\n=== Download Models ===\")\n",
    "    # download_models()\n",
    "    print(\"[Download simulation - uncomment download_models() to actually download]\")\n",
    "    \n",
    "    # Test a model\n",
    "    print(\"\\n=== Model Testing ===\")\n",
    "    if current_models:\n",
    "        test_model = current_models[0]\n",
    "        print(f\"Testing model: {test_model}\")\n",
    "        # get_model_info(test_model)\n",
    "        print(f\"[Would test {test_model} if available]\")\n",
    "\n",
    "# Uncomment to run demo\n",
    "# demo_model_management()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9886d",
   "metadata": {},
   "source": [
    "## Image Processing\n",
    "\n",
    "### Vision Model Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-to-text conversion\n",
    "import sys\n",
    "import os\n",
    "import ollama\n",
    "\n",
    "def image_to_text(image_path: str, model='llava:7b'):\n",
    "    \"\"\"\n",
    "    Convert an image to descriptive text using a vision model.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        model: Vision model to use (e.g., 'llava:7b', 'gemma3')\n",
    "    \n",
    "    Returns:\n",
    "        String description of the image\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Initial detailed description\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'user', \n",
    "                'content': 'Describe this image in detail, focusing on any text, charts, or important visual elements.',\n",
    "                'images': [image_path]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(\"Generating detailed description...\", file=sys.stderr)\n",
    "        response1 = ollama.chat(model=model, messages=messages)\n",
    "        messages.append(response1.message)\n",
    "        \n",
    "        # Request concise summary\n",
    "        messages.append({\n",
    "            'role': 'user',\n",
    "            'content': 'Create a concise, informative summary of the image in one paragraph. Focus on the most important information without introductory phrases.'\n",
    "        })\n",
    "        \n",
    "        print(\"Creating summary...\", file=sys.stderr)\n",
    "        response2 = ollama.chat(model=model, messages=messages)\n",
    "        \n",
    "        # Debug output\n",
    "        for message in messages:\n",
    "            print(f\"Message: {message}\", file=sys.stderr)\n",
    "        \n",
    "        return response2.message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "def process_image_dataset(image_directory: str, output_directory: str, model='llava:7b'):\n",
    "    \"\"\"\n",
    "    Process a directory of images, converting each to text descriptions.\n",
    "    \n",
    "    Args:\n",
    "        image_directory: Directory containing images\n",
    "        output_directory: Directory to save text descriptions\n",
    "        model: Vision model to use\n",
    "    \"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Common image extensions\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}\n",
    "    \n",
    "    image_files = []\n",
    "    for filename in os.listdir(image_directory):\n",
    "        if any(filename.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_files.append(filename)\n",
    "    \n",
    "    print(f\"Found {len(image_files)} image files to process\")\n",
    "    \n",
    "    processed = 0\n",
    "    errors = 0\n",
    "    \n",
    "    for filename in image_files:\n",
    "        image_path = os.path.join(image_directory, filename)\n",
    "        output_path = os.path.join(output_directory, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "        \n",
    "        print(f\"Processing {filename}... ({processed+1}/{len(image_files)})\")\n",
    "        \n",
    "        try:\n",
    "            description = image_to_text(image_path, model)\n",
    "            if description:\n",
    "                with open(output_path, 'w') as f:\n",
    "                    f.write(f\"# Image Description: {filename}\\n\\n\")\n",
    "                    f.write(description)\n",
    "                processed += 1\n",
    "                print(f\"✓ Saved description to {output_path}\")\n",
    "            else:\n",
    "                print(f\"✗ Failed to generate description for {filename}\")\n",
    "                errors += 1\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {filename}: {e}\")\n",
    "            errors += 1\n",
    "    \n",
    "    print(f\"\\n--- Processing Summary ---\")\n",
    "    print(f\"Processed: {processed}\")\n",
    "    print(f\"Errors: {errors}\")\n",
    "    print(f\"Total: {len(image_files)}\")\n",
    "\n",
    "# Example usage\n",
    "def demo_image_processing():\n",
    "    # Create a sample scenario\n",
    "    print(\"=== Image Processing Demo ===\")\n",
    "    print(\"This demo shows how to convert images to text descriptions.\")\n",
    "    print(\"\\nTo use this functionality:\")\n",
    "    print(\"1. Ensure you have a vision model installed (e.g., 'ollama pull llava:7b')\")\n",
    "    print(\"2. Place your images in a directory\")\n",
    "    print(\"3. Run the processing function\")\n",
    "    \n",
    "    # Simulated usage example\n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Process a single image\")\n",
    "    print(\"description = image_to_text('path/to/image.jpg')\")\n",
    "    print(\"print(description)\")\n",
    "    print()\n",
    "    print(\"# Process entire directory\")\n",
    "    print(\"process_image_dataset('images/', 'text_descriptions/')\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    # If you have an actual image, uncomment below:\n",
    "    # description = image_to_text('sample_image.jpg')\n",
    "    # print(f\"Description: {description}\")\n",
    "\n",
    "# Uncomment to run demo\n",
    "# demo_image_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1fa50",
   "metadata": {},
   "source": [
    "## Chatbot Example\n",
    "\n",
    "The following demonstrates how to make a simple REPL (Read-Eval-Print Loop) for the RAG system, combining all the components we've built throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAG chatbot implementation\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "# Configuration\n",
    "EMBED_MODEL = \"nomic-embed-text\" \n",
    "GENERATE_MODEL = os.environ.get(\"GENERATE_MODEL\", \"llama3.3\")\n",
    "\n",
    "# Global state\n",
    "client = chromadb.Client()\n",
    "collection = None\n",
    "\n",
    "def initialize_rag_system(embeddings_path=\"embed2\"):\n",
    "    \"\"\"Initialize the RAG system with document embeddings\"\"\"\n",
    "    global collection\n",
    "    \n",
    "    print(\"🚀 Initializing RAG system...\")\n",
    "    \n",
    "    # Create collection\n",
    "    collection = client.create_collection(name=\"financial_docs\")\n",
    "    \n",
    "    # Load documents\n",
    "    if not os.path.exists(embeddings_path):\n",
    "        print(f\"❌ Error: Embeddings directory '{embeddings_path}' not found!\")\n",
    "        print(\"Please run the document processing pipeline first:\")\n",
    "        print(\"1. Run: ./get-pdfs.sh\")  \n",
    "        print(\"2. Run: ./generate.sh\")\n",
    "        return False\n",
    "    \n",
    "    files = [f for f in os.listdir(embeddings_path) \n",
    "             if f.endswith('.json') and not f.endswith('.marker')]\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"❌ No embedding files found in {embeddings_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"📚 Loading {len(files)} documents...\")\n",
    "    n_loaded = 0\n",
    "    \n",
    "    for filename in files:\n",
    "        try:\n",
    "            with open(os.path.join(embeddings_path, filename), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                collection.add(\n",
    "                    ids=[filename],\n",
    "                    embeddings=data['vector'][0] if 'vector' in data else data['embedding'],\n",
    "                    documents=data.get('chunk_text', data.get('text', 'No content')),\n",
    "                )\n",
    "                n_loaded += 1\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Warning: Failed to load {filename}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Successfully loaded {n_loaded} documents\")\n",
    "    return n_loaded > 0\n",
    "\n",
    "def search_documents(query, n_results=5):\n",
    "    \"\"\"Search for relevant documents using vector similarity\"\"\"\n",
    "    if not collection:\n",
    "        raise RuntimeError(\"RAG system not initialized. Call initialize_rag_system() first.\")\n",
    "    \n",
    "    # Generate search query\n",
    "    search_response = ollama.generate(\n",
    "        GENERATE_MODEL,\n",
    "        prompt=f\"\"\"Transform this user question into effective search keywords for a financial document database.\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Return only the search keywords, no explanation:\"\"\"\n",
    "    )\n",
    "    search_query = search_response[\"response\"]\n",
    "    \n",
    "    print(f\"🔍 Search query: {search_query}\")\n",
    "    \n",
    "    # Get embeddings and search\n",
    "    embedding = ollama.embed(EMBED_MODEL, search_query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=embedding[\"embeddings\"],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def answer_question(query):\n",
    "    \"\"\"Generate answer using RAG approach\"\"\"\n",
    "    try:\n",
    "        # Search for relevant documents\n",
    "        results = search_documents(query)\n",
    "        \n",
    "        if not results['documents'][0]:\n",
    "            return \"❌ I couldn't find any relevant documents to answer your question.\"\n",
    "        \n",
    "        # Show which documents are being used\n",
    "        doc_ids = results['ids'][0]\n",
    "        print(f\"📄 Using documents: {', '.join(doc_ids[:3])}{'...' if len(doc_ids) > 3 else ''}\")\n",
    "        \n",
    "        # Combine relevant documents\n",
    "        context = '\\n---\\n'.join(results['documents'][0])\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"🤔 Generating answer...\")\n",
    "        answer_response = ollama.generate(\n",
    "            GENERATE_MODEL,\n",
    "            prompt=f\"\"\"Answer the user's question based on the provided financial document excerpts. Be accurate and cite specific figures when available.\n",
    "\n",
    "Document Context:\n",
    "{context}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Answer (be specific and factual):\"\"\"\n",
    "        )\n",
    "        \n",
    "        return answer_response[\"response\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ Error processing question: {str(e)}\"\n",
    "\n",
    "def start_chatbot():\n",
    "    \"\"\"Start the interactive RAG chatbot\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🤖 RAG CHATBOT - Financial Document Q&A\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize system\n",
    "    if not initialize_rag_system():\n",
    "        print(\"Failed to initialize RAG system. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n✨ RAG system ready!\")\n",
    "    print(\"💡 Try asking questions like:\")\n",
    "    print(\"   • What was the revenue for Q3 2024?\")\n",
    "    print(\"   • How much did the company spend on R&D?\")\n",
    "    print(\"   • What were the operating expenses?\")\n",
    "    print(\"   • What was the revenue breakdown by region?\")\n",
    "    print(\"\\n💬 Type your questions below (or 'quit' to exit)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    conversation_count = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(f\"\\n🧑 Question #{conversation_count + 1}: \").strip()\n",
    "            \n",
    "            # Check for exit commands\n",
    "            if user_input.lower() in ['quit', 'exit', 'q', 'bye']:\n",
    "                print(\"\\n👋 Thanks for using the RAG chatbot!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                print(\"Please enter a question.\")\n",
    "                continue\n",
    "            \n",
    "            # Process question and generate answer\n",
    "            start_time = time.time()\n",
    "            answer = answer_question(user_input)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Display answer\n",
    "            print(f\"\\n🤖 Answer:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(answer)\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"⏱️  Response time: {end_time - start_time:.1f}s\")\n",
    "            \n",
    "            conversation_count += 1\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n🛑 Interrupted by user\")\n",
    "            break\n",
    "        except EOFError:\n",
    "            print(\"\\n\\n👋 Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Unexpected error: {e}\")\n",
    "            print(\"Please try again or type 'quit' to exit.\")\n",
    "\n",
    "# Quick test function\n",
    "def test_rag_system():\n",
    "    \"\"\"Test the RAG system with sample questions\"\"\"\n",
    "    print(\"🧪 Testing RAG system...\")\n",
    "    \n",
    "    if not initialize_rag_system():\n",
    "        print(\"Cannot test - initialization failed\")\n",
    "        return\n",
    "    \n",
    "    test_questions = [\n",
    "        \"What was the total revenue?\",\n",
    "        \"How much was spent on research and development?\",\n",
    "        \"What were the main operating expenses?\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n--- Test {i}: {question} ---\")\n",
    "        answer = answer_question(question)\n",
    "        print(f\"Answer: {answer[:200]}{'...' if len(answer) > 200 else ''}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment one of these to run:\n",
    "    \n",
    "    # Start interactive chatbot\n",
    "    start_chatbot()\n",
    "    \n",
    "    # Or run quick test\n",
    "    # test_rag_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82107",
   "metadata": {},
   "source": [
    "### Usage Instructions\n",
    "\n",
    "#### Running the Chatbot\n",
    "\n",
    "1. **Prepare Your Data:**\n",
    "   ```bash\n",
    "   # Download dataset\n",
    "   ./get-pdfs.sh\n",
    "   \n",
    "   # Process documents (with marker-pdf and Ollama)\n",
    "   ./generate.sh\n",
    "   ```\n",
    "\n",
    "2. **Start the Chatbot:**\n",
    "   ```python\n",
    "   # Run the chatbot directly\n",
    "   start_chatbot()\n",
    "   \n",
    "   # Or test with sample questions first\n",
    "   test_rag_system()\n",
    "   ```\n",
    "\n",
    "3. **Example Interaction:**\n",
    "   ```\n",
    "   🤖 RAG CHATBOT - Financial Document Q&A\n",
    "   ============================================================\n",
    "   \n",
    "   🧑 Question #1: What was Apple's revenue in Q3 2024?\n",
    "   🔍 Search query: Apple revenue Q3 2024 earnings financial results\n",
    "   📄 Using documents: apple_10q_q3_2024.txt.json, financial_summary.txt.json\n",
    "   🤔 Generating answer...\n",
    "   \n",
    "   🤖 Answer:\n",
    "   ----------------------------------------\n",
    "   Apple's revenue for Q3 2024 was $89.5 billion, representing a 6% increase compared to Q3 2023. This growth was driven primarily by iPhone sales which contributed $46.2 billion to the total revenue.\n",
    "   ----------------------------------------\n",
    "   ⏱️  Response time: 3.2s\n",
    "   ```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Interactive REPL**: Continuous question-answering loop\n",
    "- **Smart Document Search**: Uses embeddings to find relevant information\n",
    "- **Context Display**: Shows which documents are being referenced\n",
    "- **Performance Metrics**: Displays response times\n",
    "- **Error Handling**: Graceful handling of failures and edge cases\n",
    "- **Easy Testing**: Built-in test mode for validation\n",
    "\n",
    "### Customization Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a525c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust search parameters\n",
    "results = search_documents(query, n_results=10)  # Get more documents\n",
    "\n",
    "# Use different models\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "GENERATE_MODEL = \"llama3.1:8b\"  # Use different generation model\n",
    "\n",
    "# Modify prompt for different behavior\n",
    "prompt = f\"Answer briefly and technically: {query}\\nContext: {context}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f0449",
   "metadata": {},
   "source": [
    "This chatbot implementation provides a complete, production-ready interface for interacting with your RAG system, bringing together all the components from document processing through to natural language generation."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
